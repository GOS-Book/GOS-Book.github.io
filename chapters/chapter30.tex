\part[Теория вероятностей.]{Теория вероятностей.}%\protect\linebreak \vbox{\vspace*{5\baselineskip}}\centerline{\includegraphics[height=0.3\textheight]{pictures/Foxi/teorver}}}

\chapter{Полная система событий. Формула полной вероятности. Формула Байеса.}
\section{Полная система событий}

\subsection[Классическое определение вероятности. Интуитивные понятия о вероятности.]{Классическое определение вероятности. Интуитивные понятия о вероятности.\protect\footnote{Этот и следующие билеты по теории вероятностей в ходе долгой дискуссии были одобрены преподавателем МФТИ Широбоковым Максимом. Так же рекомендую посмотреть Вам сборник определений и формулировок теорем, которые Максим выложил по этой ссылке \href{https://vk.com/shmaxg?w=wall5284431_903}{$vk.com/...$}}}

Рассмотрим обычную игральную кость --- кубик, на каждой из шести граней которого нанесены числа от 1 до 6. У нас есть всего 6 вариантов того, как этот кубик может упасть на стол после случайного бросания Какая же цифра выпадет на кубике? 
\begin{center}
1,2,3,4,5,6 --- всего 6 исходов нашего испытания.
\end{center}
Каким свойствами обладают эти варианты?
\begin{enumerate}
\item Хотя бы один из исходов обязательно случится. (т.е. на формальном языке эти исходы образуют полную группу событий )
\item Никакие два одновременно не происходят. (попарно несовместны)
\item Исходы равновозможны. (равновероятны)
\end{enumerate}

Подобных примеров можно придумать великое множество. Например, пусть есть игральная колода из 36 карт. А исход --- взаимное расположение карт друг за другом после тщательной перетасовки (их 36! всего способов переставить карты внутри множества от 1 до 36). Такая система событий тоже обладает перечисленными свойствами.

Пусть в рамках какого-то эксперимента есть какие-то исходы $w_1, \dots,w_n$, и они обладают этими тремя перечисленными выше свойствами. События, состоящее из одного исхода, станем называть \textit{элементарными событиями}. Тогда по определению считают\footnote{С современной и строгой точки зрения (Колмогорова) вероятность определяется на множествах, не на исходах, поэтому правильнее было бы элементарное событие обозначить $\{w_i\}$,т.е. как множество, состоящее из одного элемента, а вероятность $P(\{w_i\})$. Но в рамках данной книги я допущу себе эту вольность, чтобы не запутываться со строгостью изложения.}
$$
P(w_i)=\frac{1}{n}.
$$
где $P(w_i)$ --- вероятность произвольного элементарного события $w_i$. Действительно, если хотя бы один из этих исходов произойдет, причем на самом деле ровно один и эти исходы равновозможны, естественно сказать, что вероятность каждого их этих исходов --- это $1/n$. Тогда в задаче про кубик $P(w_i) =\frac{1}{6}$, а в задаче про карты $P(w_i) = \frac{1}{36!}$.

Наряду с элементарными событиями рассматриваются также случайные события, ведь часто представляет интерес наступление при испытании не какого-то элементарного события, а одного из нескольких элементарных событий.  Например, в качестве события в задаче про игральную кость можно рассмотреть $A$ --- игральная кость выпала четной стороной кверху. Это, очевидно, означает, что она выпала либо стороной 2, либо стороной 4, либо стороной 6. То есть, как говорят, элементарных исходов, которые благоприятствуют этому \textit{случайному событию} $A$ --- их всего 3 штуки. И все эти элементарные исходы равновероятны. Событие $A$ с точки зрения множеств считают множеством из всех элементарных исходов $w_{i_1},\dots w_{i_k}$, ему благоприятствующих. Тогда
\begin{defn}(Классическое определение вероятности)
\textit{Вероятностью случайного события $A$}, обозначаемой $P(A)$, называется отношение 
$$
P(A)=\frac{\left\{\parbox{7cm}{\centering число несовместимых и равновозможных\\ элементарных событий, составляющих \textit{А}}\right\}}{\{\parbox{7cm}{число всех возможных элементарных событий}\}}=\frac{k}{n}
$$ 
\end{defn}

В нашем случае $P(A)=\frac{3}{6}=\frac{1}{2}$.

Станем рассматривать некоторую систему событий $A,B,C\dots $, каждое из которых должно \textit{произойти} или \textit{не произойти}. Тогда заметим, что вероятность, определенная нами в таком смысле, обладает следующими свойствами.
\begin{enumerate}
\item 
Для каждого события $P(A)=\frac{k}{n}\ge 0$.
\item \label{ch30r2}
Обозначим за $\Omega \triangleq \{w_1, \dots,w_n\}$ --- \textit{множество (пространство) всех элементарных исходов}. Тогда, очевидно, $P(\Omega)=\frac{n}{n}=1$. Будем называть событие $\Omega$ \textit{достоверным}. 

\item 
$P(A\sqcup B)=\frac{k'+k''}{n}=\frac{k'}{n}+\frac{k''}{n}=P(A)+P(B)$ --- вероятность дизъюнктного объединения двух событий (т.е. события предполагаются непересекающимися --- элементарные исходы, благоприятствующие событию $A$, и исходы, благоприятствующие событию $B$, представляют собой непересекающиеся множества) равна сумме вероятностей события $A$ и события $B$.
\item \label{ch30r5}
$P(A\cup B)=P(A)+P(B)-P(A\cap B)$ --- вероятность обычного объединения (предполагаем, что пересечения тоже могут быть --- есть какие-то исходы, которые благоприятствуют и событию $A$, и событию $B$), т.е. тех событий, которые благоприятствуют или событию A, или событию B. Если $A\cap B=\emptyset$, то события называют \textit{несовместимыми}. И тогда, как и в предыдущем пункте, $P(A\cup B)=P(A)+P(B)$
\item
Будем обозначать любое \textit{невозможное} событие $\emptyset$. Тогда $P(\emptyset)=0$. 

\item
$\overline{A}=\Omega\setminus A$~--- \textit{отрицание события $A$}. Событие, \textit{противоположное A}, --- это событие, которому благоприятствуют те исходы, которые не благоприятствуют событию $A$. 

Тогда $P(\overline{A})=1-P(A)$.
\begin{proof}
$\overline{A}\cup A=\Omega\xrightarrow{\ref{ch30r2}}P(A\cup\overline{A}) = 1$, а так как $\overline{A}$ и $A$ несовместны, то по свойству \ref{ch30r5}:  $P(A)+P(\overline{A}) = 1$.
\end{proof}

\item
Если событие $A$ влечет за собой событие $B$ (т.е. множество $A$ является подмножеством множества $B$), то $P(A)\le P(B)$.
\begin{proof}
$P(B)=P(A\cup (\overline{A}\cap B))=P(A)+P(\overline{A}\cap B)\ge P(A).$
\end{proof}

\item
$P(A_1\cup\dots\cup A_k) \le P(A_1)+\dots+P(A_k)$.

\item Формула вклю\-чений-исключений:
\begin{multline*}
P(A_1\cup\dots\cup A_k)= P(A_1)+\dots+P(A_k)-P(A_1\cap A_2)-\\-\dots -P(A_{k-1}\cap A_{k})+ \dots + (-1)^{k-1}P(A_1\cap\dots\cap A_k)
\end{multline*}
Это полный аналог формулы вклю\-чений-исключений из комбинаторики, поэтому мы ее не доказываем.
\end{enumerate}

\begin{defn}
События $A_1, A_2,  \dots, A_n$ образуют \textit{полную группу событий}, если хотя бы одно из них непременно должно произойти, т.е. если
$$
A_1\cup A_2\cup\dots\cup A_n=\Omega.
$$
\end{defn}
Для нее верны все свойства, указанные выше.

\subsection{Аксиоматическое определение вероятности А.Н.~Колмогорова}

Аксиоматическое определение вероятности включает в себя как частные случаи классическое определение вероятности, которое мы обсудили в прошлом параграфе, и статистическое (в котором сначала проводят $n$ испытаний, и при $k$ успешных испытаниях считают $P=\lim_{n \to +\infty}\limits\frac{k(n)}{n}$) и преодолевает недостаточность каждого из них.

Отправным пунктом аксиоматики Колмогорова является множество $\Omega$, элементы которого называются \textit{элементарными событиями}. Наряду с $\Omega$ рассматривается множество $\mathfrak{F}$ подмножеств $\Omega$ --- множества элементарных событий. Элементы $\mathfrak{F}$ называются \textit{случайными событиями.}
\begin{defn} Множество $\mathfrak{F}$ называется \textit{алгеброй множеств}, если выполнены следующие требования:
\begin{enumerate}
\item
$\Omega \in \mathfrak{F},\; \varnothing \in \mathfrak{F} \;(\varnothing - \text{пустое множество});$
\item
из того, что $A \in \mathfrak{F}$, следует, что так же $\overline{A} \in \mathfrak{F}$;
\item
из того, что  $A \in \mathfrak{F}$ и $B \in \mathfrak{F}$ , следует, что $A \cup B \in \mathfrak{F}$ и $A \cap B \in \mathfrak{F}$.

Если дополнительно к перечисленным выполняется еще следующее требование:
\item
из того, что $A_n \in \mathfrak{F}$ (при $n = 1,2, \ldots$), вытекает, что $\bigcup\limits_{n} A_n \in \mathfrak{F}$ и $\bigcap\limits_{n} A_n \in \mathfrak{F}$, то множество $\mathfrak{F}$ называется \textit{$\sigma$-алгеброй}. 
\end{enumerate}
\end{defn}

Под операциями над случайными событиями понимаются операции над соответствующими множествами. 

Теперь мы можем перейти к формулировке аксиом, определяющих вероятность.
\begin{axiome} 
Каждому случайному событию $A$ поставлено в соответствие неотрицательное число $P(A)$, называемое его вероятностью.
\end{axiome}
\begin{axiome} 
$P(\Omega) = 1$
\end{axiome}
\begin{axiome}[сложения] 
Если события $A_1,A_2, \ldots, A_n$ попарно несовместимы, то 
$$
P(A_1 \cup A_2 \cup \ldots \cup A_n) = P(A_1) + P(A_2) + \ldots + P(A_n).
$$
\end{axiome}
Для классического определения вероятности свойства, выраженные аксиомами 2 и 3, не нужно было постулировать, так как эти свойства вероятности были нами доказаны.

Из сформулированных аксиом мы выведем несколько важных элементарных следствий.

Прежде всего, из очевидного равенства
$$
\Omega = \varnothing + \Omega
$$

и аксиомы 3 мы заключаем, что
$$
P(\Omega) = P(\varnothing) + P(\Omega).
$$

Таким образом.
\begin{enumerate}
\item
Вероятность невозможного события равна нулю.
\item	
Для любого события $A$
$$
P(\overline{A}) = 1 - P(A).
$$
\item
Каково бы ни было случайное событие $A$,
$$
0 \le P(A) \le 1.
$$
\item
Если событие $A$ влечет за собой событие $B$, то
$$
P(A) \le P(B).
$$
\item
Пусть $A$ и $B$ --- два произвольных события. Поскольку в суммах $A \cup B = A \cup (B \setminus (A\cap B))$ и $B = A\cap B \cup (B \setminus (A\cap B))$ слагаемые являются несовместными событиями, то в соответствии с аксиомой 3
$$
P(A \cup B) = P(A) + P(B \setminus A\cap B);\quad P(B) = P(A\cap B) + P(B \setminus A\cap B).
$$
\end{enumerate}

Отсюда вытекает теорема сложения для произвольных событий $A$ и $B$
$$
P(A \cup B) = P(A) + P(B) - P(A\cap B).
$$
В силу неотрицательности $P(A\cap B)$ отсюда заключаем, что
$$
P(A \cup B) \le P(A) + P(B).
$$
По индукции теперь выводим, что если $A_1,A_2, \ldots,A_n$ --- произвольные события, то имеет место неравенство
$$
P\{ A_1 \cup A_2 \cup \ldots \cup A_n \} \le P(A_1) + P(A_2) + \ldots + P(A_n).
$$
\begin{axiome}[Расширенная аксиома сложения]
 Если событие $A$ равносильно наступлению хотя бы одного из попарно несовместимых событий $A_1,A_2,\ldots,A_n,\ldots$, то
$$
P(A) = P(A_1) + P(A_2) + \ldots + P(A_n) + \ldots \: .
$$
\end{axiome}
Вероятностным пространством принято называть тройку символов $(\Omega, \mathfrak{F}, P)$, где $\Omega$ --- множество элементарных событий, $\mathfrak{F}$ — $\sigma$-алгебра подмножеств $\Omega$, называемых случайными событиями, и $P(A)$ --— вероятность, определенная на $\sigma$-алгебре $\mathfrak{F}$.

\section{Формула полной вероятности}
\subsection{Условная вероятность, независимость событий}

Однако в ряде случаев приходится рассматривать вероятности событий при дополнительном условии, что произошло некоторое событие В. Такие вероятности мы будем называть \textit{условными} и обозначать символом $P(A|B)$: это означает вероятность события $A$ при условии, что событие $B$ произошло.
 
Решим задачу нахождения условной вероятности для классического определения вероятности.

Пусть есть множество элементарных исходов $\Omega=\{w_1,\dots w_n\}$, событие $B=\{w_{i_1},\dots w_{i_k}\} \subseteq \Omega$. И есть еще событие $A$, вероятность которого мы хотим посчитать, при том условии, что событие $B$ произошло -- т.е. произошел ровно один из $|B|=k$ элементарных исходов, и $|B|$ надо поставить в знаменатель (столько всего возможных исходов в нашем испытании). А в числитель надо поставить количество тех благоприятствующих событию $B$ исходов, которые, в свою очередь, благоприятствуют событию $A$, т.е. найти исходы, которые благоприятствуют обоим событиям, что, естественно, равняется $|A\cap B|$.
\begin{equation} \label{ch30.2eq1}
P(A|B) = \frac{|A\cap B|}{|B|} = \frac{|A\cap B|/|\Omega|}{|B|/|\Omega|} = \frac{P(A\cap B)}{P(B)}.
\end{equation}

Понятно, что если $B$ --- событие, для которого $P(B)=0$, то равенство~$\eqref{ch30.2eq1}$ теряет смысл.

Заметим, что рассуждения, проведенные нами, не являются доказательством, а представляюет только мотивировки следующего определения.
Формула $\eqref{ch30.2eq1}$, которая в случае классического определения была нами выведена из определения условной вероятности, в случае аксиоматического определения вероятности будет взята нами в качестве определения. 
\begin{defn} В общем случае при $P(B) > 0$ по определению
$$
P(A|B) = \frac{P(A\cap B)}{P(B)}.
$$
\end{defn}

При $P(A)P(B) > 0$ равенство $\eqref{ch30.2eq1}$ эквивалентно так называемой \textit{теореме умножения}, согласно которой

\begin{equation} \label{ch30.2eq2}
P(A\cap B) = P(B|A)P(A) = P(A|B)P(B),
\end{equation}
т.е. \textit{вероятность произведения двух событий равна произведению вероятностей одного из этих событий на условную вероятность другого при условии, что первое произошло}.

Теорема умножения применима и в том случае, когда для одного из событий $A$ или $B$ вероятность равна нулю, так как в этом случае вместе, например, с $P(A) = 0$ имеют место равенства $P(A|B) = 0$ и $P(A\cap B) = 0$.

Вполне естественно, говорят, что событие $A$ \textit{независимо} от события $B$, если имеет место равенство

\begin{equation} \label{ch30.2eq3}
P(A|B) = P(A),
\end{equation}
т.е. если наступление события $B$ не изменяет вероятности события $A$. Если событие $A$ независимо от $B$, то в силу $\eqref{ch30.2eq2}$ имеет место равенство
$$
P(A)P(B|A) = P(B)P(A).
$$

Отсюда при $P(A) > 0$ находим, что
\begin{equation} \label{ch30.2eq4}
P(B|A) = P(B).
\end{equation}
т.е. событие $B$ также независимо от $A$. Таким образом, свойство независимости событий \textit{взаимно}.

Для независимых событий теорема умножения принимает особенно простой вид, а именно, если события $A$ и $B$ независимы, то
$$
P(A\cap B) = P(A) \cdot P(B).
$$

Если независимость событий $A$ и $B$ определить посредством последнего равенства, то это определение верно всегда, в том числе и тогда, когда $P(A) = 0$ или $P(B) = 0$. Поэтому
\begin{defn}
События $A$ и $B$ \textit{независимы}, если выполняется $$P(A\cap B) = P(A)\cdot P(B).$$
\end{defn}

События $B_1,B_2, \ldots, B_s$ называют \textit{независимыми в совокупности}, если для любого события $B_p$ из их числа и произвольных $B_{i_1}, B_{i_2}, \ldots, B_{i_r}$ из их же числа и отличных от $B_p$  ($i_n \not= p$  и $1 \le n \le r$) события $B_p$ и $B_{i_1}\cap B_{i_2}\cap \ldots\cap B_{i_r}$ взаимно независимы.

В силу предыдущего, это определение эквивалентно следующему:
\begin{defn} События $B_1,B_2, \ldots, B_s$ называют \textit{независимыми в совокупности}, если при любых $1 \le i_1 < i_2 < \ldots < i_r \le s$ и $r (1 \le r \le s)$
$$
P(B_{i_1}\cap B_{i_2}\cap\ldots\cap B_{i_r}) = P(B_{i_1})P(B_{i_2}) \ldots P(B_{i_r}).
$$
\end{defn}
Заметим, что для независимости в совокупности нескольких событий недостаточно их попарной независимости. Однако из независимости в совокупности вытекает попарная независимость, потройная, и т.д.

\subsection{Формула полной вероятности}
Предположим теперь, что событие $B$ может осуществиться с одним и только с одним из $n$ несовместимых событий $A_1,A_2, \ldots, A_n$. Иными словами, положим, что
\begin{equation} \label{ch30.2eq5}
B = \bigcup\limits_{i = 1}^{n} B \cap A_i,
\end{equation}
где события $B\cap A_i$ и $B\cap A_j$ с разными индексами $i$ и $j$ несовместимы. По теореме сложения вероятностей имеем:
$$
P(B) = \sum_{i = 1}^{n} P(B\cap A_i).
$$

Использовав теорему умножения, находим, что
$$
P(B) = \sum_{i = 1}^{n} P(B|A_i)P(A_i).
$$

Это равенство носит название \textit{формулы полной вероятности}.

\section{Формула Байеса}
Пусть по-прежнему имеет место равенство $\eqref{ch30.2eq5}$. Требуется найти вероятность события $A_i$, если известно, что $B$ произошло. Согласно теореме умножения имеем:
$$
P(A_i\cap B) = P(B)P(A_i|B) = P(A_i)P(B|A_i).
$$

Отсюда
$$
P(A_i|B) = \frac{P(A_i)P(B|A_i)}{P(B)},
$$
используя формулу полной вероятности, находим, что
$$
P(A_i|B) = \frac{P(A_i)P(B|A_i)}{\sum\limits_{j = 1}^{n} P(A_j)P(B|A_j)}.
$$
Два последних равенства носят название \textit{формул Байеса}.
